{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348f8abe-4600-432c-afa0-4035553c7911",
   "metadata": {},
   "source": [
    "### Домашнее задание 4 - 10 баллов\n",
    "\n",
    "В этом задании вам предстоит дообучить трансформерную модель для NER-задачи в различных форматах:\n",
    "\n",
    "1. Обучите NER-модель\n",
    "\n",
    "- Загрузите набор данных [Collection5](https://github.com/natasha/corus?tab=readme-ov-file#load_ne5) - **1 балл**\n",
    "- Разбейте набор данных на train/test части\n",
    "- Дообучите модель [rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2) на train-части корпуса для решения NER-задачи, сделайте замеры качества NER-метрик до и после дообучения - **2 балла**\n",
    "\n",
    "2. Попробуйте улучшить качество модели следующими способами:\n",
    "- Предварительно дообучите на train-части в MLM режиме, а потом дообучите на NER-задачу - **2 балла**\n",
    "- Сгенерируйте синтетическую разметку* подходящего**, на ваш взгляд, новостного корпуса большой и умной моделью для русскоязычного NER***, а затем использовав ее для дообучения rubert-tiny2 вместе с основным набором данных - **2 балла**\n",
    "\n",
    "3. Финально сравните результаты различных подходов - **1 балл**\n",
    "\n",
    "*прогоните датасет через NER-модель, получите ее предсказания и используйте их в качестве резметки\n",
    "\n",
    "**Можно использовать уже знакомый вам датасет lenta-ru, объем данных лучше взять от 10_000 текстов\n",
    "\n",
    "***Например, можно взять модель модель DeepPavlov ner_collection3_bert. Инструкция по запуску есть в [документации](https://docs.deeppavlov.ai/en/master/features/models/NER.html)\n",
    "\n",
    "**Общее**\n",
    "\n",
    "- Принимаемые решения обоснованы (почему выбрана определенная архитектура/гиперпараметр/оптимизатор/преобразование и т.п.) - **1 балл**\n",
    "- Обеспечена воспроизводимость решения: зафиксированы random_state, ноутбук воспроизводится от начала до конца без ошибок - **1 балл**\n",
    "\n",
    "**Формат сдачи ДЗ**\n",
    "\n",
    "- Каждая домашняя работа – PR в отдельную ветку **hw_n**, где **n** - номер домашней работы\n",
    "- Добавить ментора и pacifikus в reviewers\n",
    "- Дождаться ревью, если все ок – мержим в main\n",
    "- Если не ок – вносим исправления и снова отправляем на ревью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "535120f3-4a1e-492b-9291-adc4e9886a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from seqeval.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f6035-9c84-4e38-bb2b-ea57f5c95d64",
   "metadata": {},
   "source": [
    "# Загрузим данные и подготовим разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc94fd3e-8dac-4064-9821-12a229a830c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Collection5/156.txt', 'r', encoding='utf-8') as file:\n",
    "#     content = file.read().replace('\\n','\\n ')\n",
    "# len(content.split())\n",
    "\n",
    "device = \"cuda:0\"\n",
    "SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5739aa9-58a1-4a27-8c28-1a40c7e39b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка документов: 100%|██████████████████| 816/816 [00:01<00:00, 530.51it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_bio_markup(text_file, ann_file):\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    annotations = []\n",
    "    with open(ann_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            tag_parts = parts[1].split()\n",
    "            if len(tag_parts) < 3:\n",
    "                continue\n",
    "            tag = tag_parts[0]\n",
    "            try:\n",
    "                start = int(tag_parts[1])\n",
    "                end = int(tag_parts[2])\n",
    "            except ValueError:\n",
    "                print(f\"Ошибка при обработке индексов в строке: {line}\")\n",
    "                continue\n",
    "            annotations.append({'tag': tag, 'start': start, 'end': end})\n",
    "    \n",
    "    tokens = []\n",
    "    for match in re.finditer(r'\\S+', text.replace('\\n','\\n ')):\n",
    "        token = match.group()\n",
    "        start_offset = match.start()\n",
    "        end_offset = match.end()\n",
    "        tokens.append((token, start_offset, end_offset))\n",
    "    \n",
    "    bio_labels = [\"O\"] * len(tokens)    \n",
    "    # for ann in tqdm(annotations, desc=\"Разметка BIO\"):\n",
    "    for ann in annotations:\n",
    "        matched_indices = []\n",
    "        for i, (token, token_start, token_end) in enumerate(tokens):\n",
    "            token_mid = (token_start + token_end) // 2\n",
    "            if ann['start'] <= token_mid < ann['end']:\n",
    "                matched_indices.append(i)\n",
    "        if matched_indices:\n",
    "            bio_labels[matched_indices[0]] = f\"B-{ann['tag']}\"\n",
    "            for idx in matched_indices[1:]:\n",
    "                bio_labels[idx] = f\"I-{ann['tag']}\"\n",
    "    \n",
    "    return tokens, bio_labels\n",
    "\n",
    "def process_all_documents(directory: str):\n",
    "    data = []\n",
    "\n",
    "    files = [\n",
    "        f for f in os.listdir(directory)\n",
    "        if f.endswith(\".txt\") and os.path.splitext(f)[0].isdigit()\n",
    "    ]\n",
    "    files = sorted(files, key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "    for file in tqdm(files, desc=\"Обработка документов\"):\n",
    "        text_file = os.path.join(directory, file)\n",
    "        ann_file = text_file.replace(\".txt\", \".ann\")\n",
    "        \n",
    "        if os.path.exists(ann_file):\n",
    "            tokens, bio_labels = create_bio_markup(text_file, ann_file)\n",
    "            tokens = [token[0] for token in tokens]\n",
    "            data.append((tokens, bio_labels))\n",
    "        else:\n",
    "            print(f\"Предупреждение: не найден файл разметки для {file}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = process_all_documents('Collection5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e262550-f643-4d7c-b63f-a5d458d31c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens', 'ner_tags'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataset(data):\n",
    "    all_words = []\n",
    "    all_labels = []\n",
    "\n",
    "    for tokens, tags in data:\n",
    "        words = [str(t) for t in tokens]\n",
    "        labels = [str(l) for l in tags]\n",
    "        \n",
    "        all_words.append(words)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return {\"tokens\": all_words, \"ner_tags\": all_labels}\n",
    "\n",
    "\n",
    "dataset = prepare_dataset(data)\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15f9886a-ae66-451d-a51e-af290b4209c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_dict(prepare_dataset(train_data))\n",
    "test_dataset = Dataset.from_dict(prepare_dataset(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9e9f5-95d9-422e-8744-450abafc958b",
   "metadata": {},
   "source": [
    "# Дообучение модели rubert-tiny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ea9bb4b-4866-4167-8e92-e7e2d6464e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "label_list = sorted(list(set(label for ex in dataset['ner_tags'] for label in ex)))\n",
    "\n",
    "label2id = {\n",
    "    'O': 0,\n",
    "    'B-GEOPOLIT': 1,\n",
    "    'I-GEOPOLIT': 2,\n",
    "    'B-MEDIA': 3,\n",
    "    'I-MEDIA': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "    'B-ORG': 7,\n",
    "    'I-ORG': 8,\n",
    "    'B-PER': 9,\n",
    "    'I-PER': 10,\n",
    "}\n",
    "\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc19324b-c643-4ada-a607-55ec8048ddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2label.keys()), len(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc208f81-c041-466a-8da4-1c21c40d8631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc6c818a59441ba83bf43cb41064a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11082e29f94418e9875cd6ec8c3796a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# создадим label для каждого токена\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_tok = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset_tok = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "babcaa21-94bb-454d-9deb-6355c096364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tags_to_ids(dataset, label2id):    \n",
    "    for example in dataset:\n",
    "        return [[label2id[label] for label in example[\"ner_tags\"]] for example in dataset]\n",
    "\n",
    "train_num_ner_tags = convert_tags_to_ids(train_dataset, label2id)\n",
    "test_num_ner_tags = convert_tags_to_ids(test_dataset, label2id)\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "for example in train_num_ner_tags:\n",
    "    all_labels.extend([label for label in example if label != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a02eb675-e393-495a-ba21-22946e03c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(list(set(label2id.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d954e513-cdce-441d-8863-2b6147c50957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим кастомную метрику для Trainer\n",
    "\n",
    "\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "loss_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=all_labels)\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \n",
    "        labels = inputs.pop(\"labels\").to(device)  \n",
    "        \n",
    "        outputs = model(**inputs)                \n",
    "        logits = outputs.get(\"logits\").to(device)  \n",
    "\n",
    "        \n",
    "        if labels is not None:\n",
    "            weights = loss_weights\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=torch.Tensor(weights)).to(device)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(p):        \n",
    "        predictions, labels = p.predictions, p.label_ids\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        _TAGS = list(id2label.values())\n",
    "        \n",
    "        true_labels = [\n",
    "            [_TAGS[l] for (p, l) in zip(pred, label) if l != -100] \n",
    "            for pred, label in zip(preds, labels)\n",
    "        ]\n",
    "        true_preds = [\n",
    "            [_TAGS[p] for (p, l) in zip(pred, label) if l != -100] \n",
    "            for pred, label in zip(preds, labels)\n",
    "        ]\n",
    "\n",
    "        results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "        \n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ceca3fa6-fdf5-4a75-9c71-2661f7e4fd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"exp1-ner-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tok,\n",
    "    eval_dataset=test_dataset_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "47d8de00-e2f3-41fd-92e1-1635e7dd5f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Before fine-tuning ==\n",
      "{'precision': 0.01004879662558928, 'recall': 0.055064581917063225, 'f1': 0.01699597831788774, 'accuracy': 0.1335840684019698}\n"
     ]
    }
   ],
   "source": [
    "ts_ds=test_dataset_tok.remove_columns([\"tokens\", \"ner_tags\"])\n",
    "\n",
    "outputs = trainer.predict(ts_ds)\n",
    "print(\"== Before fine-tuning ==\")\n",
    "print(compute_metrics(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "545dcee9-a221-4215-bbfd-38eb4dcdb25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 01:00, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.345500</td>\n",
       "      <td>2.120201</td>\n",
       "      <td>0.046588</td>\n",
       "      <td>0.287559</td>\n",
       "      <td>0.080185</td>\n",
       "      <td>0.195357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.064900</td>\n",
       "      <td>1.827567</td>\n",
       "      <td>0.114644</td>\n",
       "      <td>0.493542</td>\n",
       "      <td>0.186066</td>\n",
       "      <td>0.466421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.829600</td>\n",
       "      <td>1.571372</td>\n",
       "      <td>0.226326</td>\n",
       "      <td>0.598459</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>0.709643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.620300</td>\n",
       "      <td>1.349983</td>\n",
       "      <td>0.300841</td>\n",
       "      <td>0.656696</td>\n",
       "      <td>0.412644</td>\n",
       "      <td>0.782835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.422800</td>\n",
       "      <td>1.164648</td>\n",
       "      <td>0.355865</td>\n",
       "      <td>0.712214</td>\n",
       "      <td>0.474594</td>\n",
       "      <td>0.819335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.248900</td>\n",
       "      <td>1.022465</td>\n",
       "      <td>0.386856</td>\n",
       "      <td>0.740313</td>\n",
       "      <td>0.508166</td>\n",
       "      <td>0.837518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.139400</td>\n",
       "      <td>0.909855</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.779062</td>\n",
       "      <td>0.542015</td>\n",
       "      <td>0.847773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.015400</td>\n",
       "      <td>0.832369</td>\n",
       "      <td>0.428171</td>\n",
       "      <td>0.796284</td>\n",
       "      <td>0.556894</td>\n",
       "      <td>0.852671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.938400</td>\n",
       "      <td>0.776456</td>\n",
       "      <td>0.438878</td>\n",
       "      <td>0.811919</td>\n",
       "      <td>0.569770</td>\n",
       "      <td>0.856459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>0.739672</td>\n",
       "      <td>0.446211</td>\n",
       "      <td>0.820530</td>\n",
       "      <td>0.578065</td>\n",
       "      <td>0.857974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.804400</td>\n",
       "      <td>0.716679</td>\n",
       "      <td>0.454070</td>\n",
       "      <td>0.826649</td>\n",
       "      <td>0.586165</td>\n",
       "      <td>0.861302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.799800</td>\n",
       "      <td>0.710163</td>\n",
       "      <td>0.453777</td>\n",
       "      <td>0.827555</td>\n",
       "      <td>0.586149</td>\n",
       "      <td>0.861058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=132, training_loss=1.2967165907224019, metrics={'train_runtime': 61.0715, 'train_samples_per_second': 128.112, 'train_steps_per_second': 2.161, 'total_flos': 132073898319072.0, 'train_loss': 1.2967165907224019, 'epoch': 12.0})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "778caef8-92dc-4f35-afaa-20ecbe1683eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== After fine-tuning ==\n",
      "{'precision': 0.4537773359840954, 'recall': 0.8275549512803082, 'f1': 0.5861487842067249, 'accuracy': 0.8610584988365171}\n"
     ]
    }
   ],
   "source": [
    "ts_ds=test_dataset_tok.remove_columns([\"tokens\", \"ner_tags\"])\n",
    "\n",
    "outputs = trainer.predict(ts_ds)\n",
    "print(\"== After fine-tuning ==\")\n",
    "print(compute_metrics(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b52c39-050a-46ac-87b1-533837f9682f",
   "metadata": {},
   "source": [
    "# Попробуйте улучшить качество модели следующими способами:\n",
    "- Предварительно дообучите на train-части в MLM режиме, а потом дообучите на NER-задачу - 2 балла\n",
    "- Сгенерируйте синтетическую разметку* подходящего**, на ваш взгляд, новостного корпуса большой и умной моделью для русскоязычного NER***, а затем использовав ее для дообучения rubert-tiny2 вместе с основным набором данных - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa690e23-b52a-41fd-9d38-752b9624ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_text = [\" \".join(data['tokens']) for data in train_dataset]\n",
    "test_full_text = [\" \".join(data['tokens']) for data in test_dataset]\n",
    "\n",
    "train_mlm_dataset = Dataset.from_dict({\"text\": train_full_text})\n",
    "test_mlm_dataset = Dataset.from_dict({\"text\": test_full_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "54978a2f-f3fe-420b-9914-4e2032bb0cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f34cf30929461aa68b7d1a3ace777d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2308 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c76fcf5413e4ccf9d14399522ec1f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_mlm(example):\n",
    "    return tokenizer(example['text'], return_special_tokens_mask=True)\n",
    "\n",
    "train_tokenized_mlm_dataset = train_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])\n",
    "test_tokenized_mlm_dataset = test_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a98d7c7-b6b4-43e3-aba2-a77bbc49f5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395f30a404704946ab1d45f14d5e1d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e22c03871254e13ad18862def181b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "train_mlm_dataset = train_tokenized_mlm_dataset.map(group_texts, batched=True)\n",
    "test_mlm_dataset = test_tokenized_mlm_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c606c247-78b2-4cea-810e-08ff4225a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 01:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.093219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.112690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.034633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.034324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.983469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.952465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.969285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.995662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.046781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.978892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=3.2770406087239583, metrics={'train_runtime': 77.614, 'train_samples_per_second': 99.982, 'train_steps_per_second': 1.16, 'total_flos': 29611075338240.0, 'train_loss': 3.2770406087239583, 'epoch': 10.0})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=\"./mlm-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=train_mlm_dataset,\n",
    "    eval_dataset=test_mlm_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "70c1e566-b2a4-41c4-9176-7ce3b42f27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 21.24\n"
     ]
    }
   ],
   "source": [
    "eval_result = trainer_mlm.evaluate()\n",
    "print(f\"Perplexity: {np.exp(eval_result['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66a1a3-9d68-4c65-a221-90278aad9d08",
   "metadata": {},
   "source": [
    "### После MLM обучим модель на задачу NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb40f4-40c3-47bb-aa03-c0065b85a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"mlm-rubert-tiny2/checkpoint-40\", \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"exp2-ner-mlm-rubert-tiny2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tok,\n",
    "    eval_dataset=test_dataset_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90054a7-9306-4196-807f-466ab07a46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f281da9-c6f1-4226-96ea-318e08a276de",
   "metadata": {},
   "source": [
    "#### Попрбуем синтезировать данные на оснве lenta-ru и снова дообучить модель через ранее пройденный pipeline ( MLM + FT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "477bd4be-8ab8-4389-b6de-08186ec33d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-crf==0.7.* in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (0.7.2)\n",
      "Ignoring transformers: markers 'python_version < \"3.8\"' don't match your environment\n",
      "Requirement already satisfied: transformers==4.30.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (4.30.0)\n",
      "Requirement already satisfied: filelock in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2024.8.30)\n",
      "Requirement already satisfied: protobuf<=3.20 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (3.20.0)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch<1.14.0,>=1.6.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (75.1.0)\n",
      "Requirement already satisfied: wheel in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (0.44.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 05:48:02.460 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/v1/ner/ner_rus_bert_coll3_torch.tar.gz download because of matching hashes\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model('ner_collection3_bert', download=True, install=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e8ad19c-0a67-486f-bcba-fa98c080869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Президент', 'России', 'Владимир', 'Путин', 'прибыл', 'в', 'Ташкент', '.']], [['O', 'S-LOC', 'B-PER', 'E-PER', 'O', 'O', 'S-LOC', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "preds = ner_model([\"Президент России Владимир Путин прибыл в Ташкент.\"])\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1200b74e-16d1-4dda-8b3d-0b714b0b3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned-lenta-ru-news-100k.csv\")\n",
    "full_text = df['full_text'][:10000].tolist()\n",
    "\n",
    "def prep_text(text:str):\n",
    "    return text.replace('\\xa0', ' ')\n",
    "\n",
    "full_text = list(map(prep_text, full_text))\n",
    "\n",
    "\n",
    "def split_text(text, max_tokens=512):\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for token in tqdm(tokens):\n",
    "        current_chunk.append(token)\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            chunks.append(tokenizer.convert_tokens_to_string(current_chunk))\n",
    "            current_chunk = []\n",
    "    if current_chunk:\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f05a94-1f55-4fcd-818a-679dcd564b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2689016 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████| 2689016/2689016 [00:02<00:00, 968454.60it/s]\n",
      "100%|███████████████████████████████████████| 5252/5252 [02:57<00:00, 29.63it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(full_text)\n",
    "\n",
    "all_preds = []\n",
    "for chunk in tqdm(chunks):\n",
    "    try:\n",
    "        preds = ner_model([chunk])\n",
    "    except:\n",
    "        continue\n",
    "    all_preds.append(preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "718e58f0-295b-41a1-9953-35df49bbc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tag(tags: list):\n",
    "    new_tag = None\n",
    "    new_tag = list(map(lambda x: x.replace('E','I').replace('S','B').replace('PIR','PER'), tags))\n",
    "    return new_tag\n",
    "\n",
    "add_dataset = {\n",
    "    'tokens': [],\n",
    "    'ner_tags': []\n",
    "}\n",
    "\n",
    "for i in all_preds:\n",
    "    ner_tag = replace_tag(i[1][0])\n",
    "    add_dataset['tokens'].append(i[0][0])\n",
    "    add_dataset['ner_tags'].append(ner_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585b6ea-0354-4134-a408-3a1f6dc29e03",
   "metadata": {},
   "source": [
    "### Перейдем к обучению на уровне MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40bb722c-7473-48b0-875c-a8e44558243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_tokens, test_data_tokens, train_data_tags, test_data_tags = train_test_split(\n",
    "    add_dataset[\"tokens\"], add_dataset[\"ner_tags\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_add_dataset = {\"tokens\": train_data_tokens, \"ner_tags\": train_data_tags}\n",
    "test_add_data_dataset = {\"tokens\": test_data_tokens, \"ner_tags\": test_data_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "800e38c1-d3cc-422c-9ca3-a154e0b53135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_add_dataset = Dataset.from_dict(train_add_dataset)\n",
    "test_add_dataset = Dataset.from_dict(test_add_data_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84b03466-3ead-4911-99c0-08fa09b05adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b783386949bf41e79a08919836d6b402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fac66af139e48d5b144e9d0af878a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_add_dataset_tok = train_add_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_add_dataset_tok = test_add_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34371085-385e-46b6-929b-c7fc4614232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-100, 0, 5, 6, 7, 8, 9, 10}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_add_dataset_tok['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97389e34-e98a-4765-a065-091de743f58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 652\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5ba29b6-8b7e-41d9-a76e-6253d9e2da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_dataset = concatenate_datasets([train_add_dataset_tok, train_dataset_tok])\n",
    "test_merged_dataset = concatenate_datasets([test_add_dataset_tok, test_dataset_tok])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcae7c-1d86-4514-9a90-9eed28b23590",
   "metadata": {},
   "source": [
    "## Обучаем на уровне MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fb04d2a-1569-420a-96bc-57c82784de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_text = [\" \".join(data['tokens']) for data in train_merged_dataset]\n",
    "test_full_text = [\" \".join(data['tokens']) for data in test_merged_dataset]\n",
    "\n",
    "train_mlm_dataset = Dataset.from_dict({\"text\": train_full_text})\n",
    "test_mlm_dataset = Dataset.from_dict({\"text\": test_full_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26b1c3af-f4f7-4685-852a-b97c36306bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d425ff579ca744f69a41bb402a2e7e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef8b2bd6f5245029bab91e48e74451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_mlm(example):\n",
    "    return tokenizer(example['text'], return_special_tokens_mask=True)\n",
    "\n",
    "train_tokenized_mlm_dataset = train_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])\n",
    "test_tokenized_mlm_dataset = test_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "007dffc1-ac0c-411d-a209-0dbb25ffec43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775cedd12a0c4f1fa1c51172ef2a8854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56b879e864249af9601113578b8a724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "train_mlm_dataset = train_tokenized_mlm_dataset.map(group_texts, batched=True)\n",
    "test_mlm_dataset = test_tokenized_mlm_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f5bc4bf-5250-4f0f-b82a-9e60448d7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=\"./exp3-mlm-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=train_mlm_dataset,\n",
    "    eval_dataset=test_mlm_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1dc0689-9cc4-45b4-acca-4ab1d946bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer_mlm.evaluate()\n",
    "print(f\"Perplexity: {np.exp(eval_result['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7c4f320-9400-413f-b765-adfc356005b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[0m\u001b[01;34mcheckpoint-1062\u001b[0m/  \u001b[01;34mcheckpoint-1770\u001b[0m/  \u001b[01;34mcheckpoint-2832\u001b[0m/  \u001b[01;34mcheckpoint-3540\u001b[0m/\n",
      "\u001b[01;34mcheckpoint-1416\u001b[0m/  \u001b[01;34mcheckpoint-2124\u001b[0m/  \u001b[01;34mcheckpoint-3186\u001b[0m/  \u001b[01;34mcheckpoint-531\u001b[0m/\n",
      "\u001b[01;34mcheckpoint-177\u001b[0m/   \u001b[01;34mcheckpoint-2478\u001b[0m/  \u001b[01;34mcheckpoint-354\u001b[0m/   \u001b[01;34mcheckpoint-708\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls exp3-mlm-rubert-tiny2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed2aa5-f711-4c44-9d52-1703e61488db",
   "metadata": {},
   "source": [
    "## Перейдем к обучению в рамках NER задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f5af2e6-a524-412c-ba1d-f2247593e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at exp3-mlm-rubert-tiny2/checkpoint-3540/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at exp3-mlm-rubert-tiny2/checkpoint-3540/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"exp3-mlm-rubert-tiny2/checkpoint-3540/\", \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"exp3-ner-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    # fp16=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_merged_dataset,\n",
    "    eval_dataset=test_merged_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "591917d7-a216-4ebb-b81d-88c078269032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2232' max='2232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2232/2232 05:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.367783</td>\n",
       "      <td>0.603602</td>\n",
       "      <td>0.845057</td>\n",
       "      <td>0.704207</td>\n",
       "      <td>0.938915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.258535</td>\n",
       "      <td>0.639106</td>\n",
       "      <td>0.866841</td>\n",
       "      <td>0.735754</td>\n",
       "      <td>0.944950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>0.691225</td>\n",
       "      <td>0.886488</td>\n",
       "      <td>0.776773</td>\n",
       "      <td>0.955109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.190953</td>\n",
       "      <td>0.733790</td>\n",
       "      <td>0.887730</td>\n",
       "      <td>0.803453</td>\n",
       "      <td>0.962428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.182828</td>\n",
       "      <td>0.750827</td>\n",
       "      <td>0.901664</td>\n",
       "      <td>0.819362</td>\n",
       "      <td>0.965762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>0.183116</td>\n",
       "      <td>0.763709</td>\n",
       "      <td>0.905961</td>\n",
       "      <td>0.828776</td>\n",
       "      <td>0.967167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.177660</td>\n",
       "      <td>0.763815</td>\n",
       "      <td>0.906731</td>\n",
       "      <td>0.829160</td>\n",
       "      <td>0.967241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.174981</td>\n",
       "      <td>0.775119</td>\n",
       "      <td>0.909811</td>\n",
       "      <td>0.837081</td>\n",
       "      <td>0.969179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.179772</td>\n",
       "      <td>0.789218</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>0.846006</td>\n",
       "      <td>0.971388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.179025</td>\n",
       "      <td>0.786179</td>\n",
       "      <td>0.911898</td>\n",
       "      <td>0.844385</td>\n",
       "      <td>0.970847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.181171</td>\n",
       "      <td>0.789326</td>\n",
       "      <td>0.913587</td>\n",
       "      <td>0.846923</td>\n",
       "      <td>0.971339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.182454</td>\n",
       "      <td>0.793317</td>\n",
       "      <td>0.914009</td>\n",
       "      <td>0.849397</td>\n",
       "      <td>0.972007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2232, training_loss=0.19359060778119017, metrics={'train_runtime': 302.0502, 'train_samples_per_second': 177.03, 'train_steps_per_second': 7.39, 'total_flos': 447207895293888.0, 'train_loss': 0.19359060778119017, 'epoch': 12.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad96a7-cd22-48af-9b02-9700a72b8a28",
   "metadata": {},
   "source": [
    "# Посмотрим что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6426788d-1fb9-4cd9-a166-f5e77fc257df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"exp3-ner-rubert-tiny2/checkpoint-2232\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "016b31c4-44be-49b1-8f53-bf359eaca087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'ORG', 'score': 0.951569, 'word': 'Сбербанка', 'start': 21, 'end': 30}\n",
      "{'entity_group': 'PER', 'score': 0.9922959, 'word': 'Герман Греф', 'start': 31, 'end': 42}\n",
      "{'entity_group': 'LOC', 'score': 0.60198957, 'word': 'Москве', 'start': 60, 'end': 66}\n",
      "{'entity_group': 'ORG', 'score': 0.6973838, 'word': 'Яндексом', 'start': 96, 'end': 104}\n",
      "{'entity_group': 'GEOPOLIT', 'score': 0.9631994, 'word': 'Российской Федерации', 'start': 203, 'end': 223}\n",
      "{'entity_group': 'ORG', 'score': 0.85091865, 'word': 'Евразийского экономического союза.', 'start': 284, 'end': 318}\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"Генеральный директор Сбербанка Герман Греф на конференции в Москве заявил, что сотрудничество с Яндексом в области искусственного интеллекта выходит на новый уровень. Он также отметил, что правительство Российской Федерации поддерживает развитие цифровой экономики, особенно в рамках Евразийского экономического союза.\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "for entity in results:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель закомител в hf: r1char9/ner-rubert-tiny-RuNews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
