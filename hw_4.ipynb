{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348f8abe-4600-432c-afa0-4035553c7911",
   "metadata": {},
   "source": [
    "### Домашнее задание 4 - 10 баллов\n",
    "\n",
    "В этом задании вам предстоит дообучить трансформерную модель для NER-задачи в различных форматах:\n",
    "\n",
    "1. Обучите NER-модель\n",
    "\n",
    "- Загрузите набор данных [Collection5](https://github.com/natasha/corus?tab=readme-ov-file#load_ne5) - **1 балл**\n",
    "- Разбейте набор данных на train/test части\n",
    "- Дообучите модель [rubert-tiny2](https://huggingface.co/cointegrated/rubert-tiny2) на train-части корпуса для решения NER-задачи, сделайте замеры качества NER-метрик до и после дообучения - **2 балла**\n",
    "\n",
    "2. Попробуйте улучшить качество модели следующими способами:\n",
    "- Предварительно дообучите на train-части в MLM режиме, а потом дообучите на NER-задачу - **2 балла**\n",
    "- Сгенерируйте синтетическую разметку* подходящего**, на ваш взгляд, новостного корпуса большой и умной моделью для русскоязычного NER***, а затем использовав ее для дообучения rubert-tiny2 вместе с основным набором данных - **2 балла**\n",
    "\n",
    "3. Финально сравните результаты различных подходов - **1 балл**\n",
    "\n",
    "*прогоните датасет через NER-модель, получите ее предсказания и используйте их в качестве резметки\n",
    "\n",
    "**Можно использовать уже знакомый вам датасет lenta-ru, объем данных лучше взять от 10_000 текстов\n",
    "\n",
    "***Например, можно взять модель модель DeepPavlov ner_collection3_bert. Инструкция по запуску есть в [документации](https://docs.deeppavlov.ai/en/master/features/models/NER.html)\n",
    "\n",
    "**Общее**\n",
    "\n",
    "- Принимаемые решения обоснованы (почему выбрана определенная архитектура/гиперпараметр/оптимизатор/преобразование и т.п.) - **1 балл**\n",
    "- Обеспечена воспроизводимость решения: зафиксированы random_state, ноутбук воспроизводится от начала до конца без ошибок - **1 балл**\n",
    "\n",
    "**Формат сдачи ДЗ**\n",
    "\n",
    "- Каждая домашняя работа – PR в отдельную ветку **hw_n**, где **n** - номер домашней работы\n",
    "- Добавить ментора и pacifikus в reviewers\n",
    "- Дождаться ревью, если все ок – мержим в main\n",
    "- Если не ок – вносим исправления и снова отправляем на ревью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535120f3-4a1e-492b-9291-adc4e9886a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from seqeval.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f6035-9c84-4e38-bb2b-ea57f5c95d64",
   "metadata": {},
   "source": [
    "# Загрузим данные и подготовим разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc94fd3e-8dac-4064-9821-12a229a830c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Collection5/156.txt', 'r', encoding='utf-8') as file:\n",
    "#     content = file.read().replace('\\n','\\n ')\n",
    "# len(content.split())\n",
    "\n",
    "device = \"cuda:0\"\n",
    "SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5739aa9-58a1-4a27-8c28-1a40c7e39b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка документов: 100%|██████████████████| 816/816 [00:01<00:00, 513.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_bio_markup(text_file, ann_file):\n",
    "    with open(text_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    annotations = []\n",
    "    with open(ann_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            tag_parts = parts[1].split()\n",
    "            if len(tag_parts) < 3:\n",
    "                continue\n",
    "            tag = tag_parts[0]\n",
    "            try:\n",
    "                start = int(tag_parts[1])\n",
    "                end = int(tag_parts[2])\n",
    "            except ValueError:\n",
    "                print(f\"Ошибка при обработке индексов в строке: {line}\")\n",
    "                continue\n",
    "            annotations.append({'tag': tag, 'start': start, 'end': end})\n",
    "    \n",
    "    tokens = []\n",
    "    for match in re.finditer(r'\\S+', text.replace('\\n','\\n ')):\n",
    "        token = match.group()\n",
    "        start_offset = match.start()\n",
    "        end_offset = match.end()\n",
    "        tokens.append((token, start_offset, end_offset))\n",
    "    \n",
    "    bio_labels = [\"O\"] * len(tokens)    \n",
    "    # for ann in tqdm(annotations, desc=\"Разметка BIO\"):\n",
    "    for ann in annotations:\n",
    "        matched_indices = []\n",
    "        for i, (token, token_start, token_end) in enumerate(tokens):\n",
    "            token_mid = (token_start + token_end) // 2\n",
    "            if ann['start'] <= token_mid < ann['end']:\n",
    "                matched_indices.append(i)\n",
    "        if matched_indices:\n",
    "            bio_labels[matched_indices[0]] = f\"B-{ann['tag']}\"\n",
    "            for idx in matched_indices[1:]:\n",
    "                bio_labels[idx] = f\"I-{ann['tag']}\"\n",
    "    \n",
    "    return tokens, bio_labels\n",
    "\n",
    "def process_all_documents(directory: str):\n",
    "    data = []\n",
    "\n",
    "    files = [\n",
    "        f for f in os.listdir(directory)\n",
    "        if f.endswith(\".txt\") and os.path.splitext(f)[0].isdigit()\n",
    "    ]\n",
    "    files = sorted(files, key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\n",
    "    for file in tqdm(files, desc=\"Обработка документов\"):\n",
    "        text_file = os.path.join(directory, file)\n",
    "        ann_file = text_file.replace(\".txt\", \".ann\")\n",
    "        \n",
    "        if os.path.exists(ann_file):\n",
    "            tokens, bio_labels = create_bio_markup(text_file, ann_file)\n",
    "            tokens = [token[0] for token in tokens]\n",
    "            data.append((tokens, bio_labels))\n",
    "        else:\n",
    "            print(f\"Предупреждение: не найден файл разметки для {file}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = process_all_documents('Collection5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e262550-f643-4d7c-b63f-a5d458d31c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens', 'ner_tags'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataset(data):\n",
    "    all_words = []\n",
    "    all_labels = []\n",
    "\n",
    "    for tokens, tags in data:\n",
    "        words = [str(t) for t in tokens]\n",
    "        labels = [str(l) for l in tags]\n",
    "        \n",
    "        all_words.append(words)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return {\"tokens\": all_words, \"ner_tags\": all_labels}\n",
    "\n",
    "\n",
    "dataset = prepare_dataset(data)\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f9886a-ae66-451d-a51e-af290b4209c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_dict(prepare_dataset(train_data))\n",
    "test_dataset = Dataset.from_dict(prepare_dataset(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9e9f5-95d9-422e-8744-450abafc958b",
   "metadata": {},
   "source": [
    "# Дообучение модели rubert-tiny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea9bb4b-4866-4167-8e92-e7e2d6464e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "label_list = sorted(list(set(label for ex in dataset['ner_tags'] for label in ex)))\n",
    "\n",
    "label2id = {\n",
    "    'O': 0,\n",
    "    'B-GEOPOLIT': 1,\n",
    "    'I-GEOPOLIT': 2,\n",
    "    'B-MEDIA': 3,\n",
    "    'I-MEDIA': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "    'B-ORG': 7,\n",
    "    'I-ORG': 8,\n",
    "    'B-PER': 9,\n",
    "    'I-PER': 10,\n",
    "}\n",
    "\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc19324b-c643-4ada-a607-55ec8048ddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2label.keys()), len(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc208f81-c041-466a-8da4-1c21c40d8631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf191d8042994805866185740da6892a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c12c0d78a954970a0f9e7ab0889556d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# создадим label для каждого токена\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_tok = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset_tok = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "babcaa21-94bb-454d-9deb-6355c096364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tags_to_ids(dataset, label2id):    \n",
    "    for example in dataset:\n",
    "        return [[label2id[label] for label in example[\"ner_tags\"]] for example in dataset]\n",
    "\n",
    "train_num_ner_tags = convert_tags_to_ids(train_dataset, label2id)\n",
    "test_num_ner_tags = convert_tags_to_ids(test_dataset, label2id)\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "for example in train_num_ner_tags:\n",
    "    all_labels.extend([label for label in example if label != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02eb675-e393-495a-ba21-22946e03c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(list(set(label2id.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d954e513-cdce-441d-8863-2b6147c50957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим кастомную метрику для Trainer\n",
    "\n",
    "\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "loss_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=all_labels)\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \n",
    "        labels = inputs.pop(\"labels\").to(device)  \n",
    "        \n",
    "        outputs = model(**inputs)                \n",
    "        logits = outputs.get(\"logits\").to(device)  \n",
    "\n",
    "        \n",
    "        if labels is not None:\n",
    "            weights = loss_weights\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=torch.Tensor(weights)).to(device)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(p):        \n",
    "        predictions, labels = p.predictions, p.label_ids\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        _TAGS = list(id2label.values())\n",
    "        \n",
    "        true_labels = [\n",
    "            [_TAGS[l] for (p, l) in zip(pred, label) if l != -100] \n",
    "            for pred, label in zip(preds, labels)\n",
    "        ]\n",
    "        true_preds = [\n",
    "            [_TAGS[p] for (p, l) in zip(pred, label) if l != -100] \n",
    "            for pred, label in zip(preds, labels)\n",
    "        ]\n",
    "\n",
    "        results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "        \n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceca3fa6-fdf5-4a75-9c71-2661f7e4fd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"exp1-ner-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tok,\n",
    "    eval_dataset=test_dataset_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d8de00-e2f3-41fd-92e1-1635e7dd5f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Before fine-tuning ==\n",
      "{'precision': 0.017217514601127578, 'recall': 0.11556764106050306, 'f1': 0.02997002997002997, 'accuracy': 0.052194382812922775}\n"
     ]
    }
   ],
   "source": [
    "ts_ds=test_dataset_tok.remove_columns([\"tokens\", \"ner_tags\"])\n",
    "\n",
    "outputs = trainer.predict(ts_ds)\n",
    "print(\"== Before fine-tuning ==\")\n",
    "print(compute_metrics(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "545dcee9-a221-4215-bbfd-38eb4dcdb25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33milike528149\u001b[0m (\u001b[33mr1char9\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/tank/scratch/rgurtsiev/workflow/wandb/run-20250414_091634-9hjiv9ly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/r1char9/huggingface/runs/9hjiv9ly' target=\"_blank\">dutiful-darkness-88</a></strong> to <a href='https://wandb.ai/r1char9/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/r1char9/huggingface' target=\"_blank\">https://wandb.ai/r1char9/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/r1char9/huggingface/runs/9hjiv9ly' target=\"_blank\">https://wandb.ai/r1char9/huggingface/runs/9hjiv9ly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 01:00, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.317700</td>\n",
       "      <td>2.106775</td>\n",
       "      <td>0.055223</td>\n",
       "      <td>0.340811</td>\n",
       "      <td>0.095046</td>\n",
       "      <td>0.225932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.038400</td>\n",
       "      <td>1.812135</td>\n",
       "      <td>0.107792</td>\n",
       "      <td>0.479946</td>\n",
       "      <td>0.176045</td>\n",
       "      <td>0.470669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.805000</td>\n",
       "      <td>1.550176</td>\n",
       "      <td>0.192437</td>\n",
       "      <td>0.585769</td>\n",
       "      <td>0.289701</td>\n",
       "      <td>0.660236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.590400</td>\n",
       "      <td>1.327061</td>\n",
       "      <td>0.265702</td>\n",
       "      <td>0.661455</td>\n",
       "      <td>0.379116</td>\n",
       "      <td>0.743168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.398300</td>\n",
       "      <td>1.144892</td>\n",
       "      <td>0.320808</td>\n",
       "      <td>0.715840</td>\n",
       "      <td>0.443058</td>\n",
       "      <td>0.789464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.223700</td>\n",
       "      <td>1.003715</td>\n",
       "      <td>0.351346</td>\n",
       "      <td>0.748470</td>\n",
       "      <td>0.478211</td>\n",
       "      <td>0.808079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.112600</td>\n",
       "      <td>0.896132</td>\n",
       "      <td>0.379584</td>\n",
       "      <td>0.781101</td>\n",
       "      <td>0.510894</td>\n",
       "      <td>0.821960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.393566</td>\n",
       "      <td>0.795604</td>\n",
       "      <td>0.526624</td>\n",
       "      <td>0.830077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.763236</td>\n",
       "      <td>0.407091</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>0.541676</td>\n",
       "      <td>0.835705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.820100</td>\n",
       "      <td>0.727122</td>\n",
       "      <td>0.413321</td>\n",
       "      <td>0.814185</td>\n",
       "      <td>0.548298</td>\n",
       "      <td>0.839142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>0.704292</td>\n",
       "      <td>0.417654</td>\n",
       "      <td>0.814865</td>\n",
       "      <td>0.552254</td>\n",
       "      <td>0.841875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.697556</td>\n",
       "      <td>0.417838</td>\n",
       "      <td>0.815318</td>\n",
       "      <td>0.552518</td>\n",
       "      <td>0.841658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=132, training_loss=1.2730676459543633, metrics={'train_runtime': 63.9156, 'train_samples_per_second': 122.411, 'train_steps_per_second': 2.065, 'total_flos': 132073898319072.0, 'train_loss': 1.2730676459543633, 'epoch': 12.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778caef8-92dc-4f35-afaa-20ecbe1683eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== After fine-tuning ==\n",
      "{'precision': 0.4178376495180583, 'recall': 0.8153183775209608, 'f1': 0.5525184275184275, 'accuracy': 0.841658098381947}\n"
     ]
    }
   ],
   "source": [
    "ts_ds=test_dataset_tok.remove_columns([\"tokens\", \"ner_tags\"])\n",
    "\n",
    "outputs = trainer.predict(ts_ds)\n",
    "print(\"== After fine-tuning ==\")\n",
    "print(compute_metrics(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b52c39-050a-46ac-87b1-533837f9682f",
   "metadata": {},
   "source": [
    "# Попробуйте улучшить качество модели следующими способами:\n",
    "- Предварительно дообучите на train-части в MLM режиме, а потом дообучите на NER-задачу - 2 балла\n",
    "- Сгенерируйте синтетическую разметку* подходящего**, на ваш взгляд, новостного корпуса большой и умной моделью для русскоязычного NER***, а затем использовав ее для дообучения rubert-tiny2 вместе с основным набором данных - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa690e23-b52a-41fd-9d38-752b9624ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_text = [\" \".join(data['tokens']) for data in train_dataset]\n",
    "test_full_text = [\" \".join(data['tokens']) for data in test_dataset]\n",
    "\n",
    "train_mlm_dataset = Dataset.from_dict({\"text\": train_full_text})\n",
    "test_mlm_dataset = Dataset.from_dict({\"text\": test_full_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54978a2f-f3fe-420b-9914-4e2032bb0cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ac06383e684c56be8c05fe8c9471d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2308 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77eb4bbd39b461693b84629ecd41274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_mlm(example):\n",
    "    return tokenizer(example['text'], return_special_tokens_mask=True)\n",
    "\n",
    "train_tokenized_mlm_dataset = train_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])\n",
    "test_tokenized_mlm_dataset = test_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a98d7c7-b6b4-43e3-aba2-a77bbc49f5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7abc7d8fa894e39ad3552e0133270c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2ad50e00bf4f9f8e2aeff2fa0a4722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "train_mlm_dataset = train_tokenized_mlm_dataset.map(group_texts, batched=True)\n",
    "test_mlm_dataset = test_tokenized_mlm_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c606c247-78b2-4cea-810e-08ff4225a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 01:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.093219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.112690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.034633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.034324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.983469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.952465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.969285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.995662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.046781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.978892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=3.2770406087239583, metrics={'train_runtime': 79.7551, 'train_samples_per_second': 97.298, 'train_steps_per_second': 1.128, 'total_flos': 29611075338240.0, 'train_loss': 3.2770406087239583, 'epoch': 10.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=\"./mlm-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=train_mlm_dataset,\n",
    "    eval_dataset=test_mlm_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c1e566-b2a4-41c4-9176-7ce3b42f27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 21.20\n"
     ]
    }
   ],
   "source": [
    "eval_result = trainer_mlm.evaluate()\n",
    "print(f\"Perplexity: {np.exp(eval_result['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66a1a3-9d68-4c65-a221-90278aad9d08",
   "metadata": {},
   "source": [
    "### После MLM обучим модель на задачу NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddb40f4-40c3-47bb-aa03-c0065b85a025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mlm-rubert-tiny2/checkpoint-40 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at mlm-rubert-tiny2/checkpoint-40 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"mlm-rubert-tiny2/checkpoint-40\", \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"exp2-ner-mlm-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tok,\n",
    "    eval_dataset=test_dataset_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b90054a7-9306-4196-807f-466ab07a46da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 00:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.269800</td>\n",
       "      <td>2.037245</td>\n",
       "      <td>0.060122</td>\n",
       "      <td>0.384999</td>\n",
       "      <td>0.104003</td>\n",
       "      <td>0.210212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.992700</td>\n",
       "      <td>1.746221</td>\n",
       "      <td>0.124706</td>\n",
       "      <td>0.540449</td>\n",
       "      <td>0.202651</td>\n",
       "      <td>0.498864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.759100</td>\n",
       "      <td>1.496771</td>\n",
       "      <td>0.204677</td>\n",
       "      <td>0.626785</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.675713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.549000</td>\n",
       "      <td>1.280530</td>\n",
       "      <td>0.280040</td>\n",
       "      <td>0.693406</td>\n",
       "      <td>0.398957</td>\n",
       "      <td>0.756399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.354200</td>\n",
       "      <td>1.105619</td>\n",
       "      <td>0.332063</td>\n",
       "      <td>0.730569</td>\n",
       "      <td>0.456593</td>\n",
       "      <td>0.795227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.210900</td>\n",
       "      <td>0.968489</td>\n",
       "      <td>0.356582</td>\n",
       "      <td>0.761160</td>\n",
       "      <td>0.485650</td>\n",
       "      <td>0.811732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.066700</td>\n",
       "      <td>0.863360</td>\n",
       "      <td>0.383687</td>\n",
       "      <td>0.785633</td>\n",
       "      <td>0.515577</td>\n",
       "      <td>0.826019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.970600</td>\n",
       "      <td>0.785594</td>\n",
       "      <td>0.411417</td>\n",
       "      <td>0.805121</td>\n",
       "      <td>0.544563</td>\n",
       "      <td>0.839737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.885400</td>\n",
       "      <td>0.732605</td>\n",
       "      <td>0.417996</td>\n",
       "      <td>0.813732</td>\n",
       "      <td>0.552292</td>\n",
       "      <td>0.842632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.793100</td>\n",
       "      <td>0.697218</td>\n",
       "      <td>0.428436</td>\n",
       "      <td>0.818717</td>\n",
       "      <td>0.562510</td>\n",
       "      <td>0.848287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.754900</td>\n",
       "      <td>0.676336</td>\n",
       "      <td>0.432439</td>\n",
       "      <td>0.821663</td>\n",
       "      <td>0.566651</td>\n",
       "      <td>0.848964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.745700</td>\n",
       "      <td>0.669357</td>\n",
       "      <td>0.435142</td>\n",
       "      <td>0.823249</td>\n",
       "      <td>0.569346</td>\n",
       "      <td>0.850425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=132, training_loss=1.2363191400513505, metrics={'train_runtime': 59.5053, 'train_samples_per_second': 131.484, 'train_steps_per_second': 2.218, 'total_flos': 130875234668400.0, 'train_loss': 1.2363191400513505, 'epoch': 12.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f281da9-c6f1-4226-96ea-318e08a276de",
   "metadata": {},
   "source": [
    "#### Попрбуем синтезировать данные на оснве lenta-ru и снова дообучить модель через ранее пройденный pipeline ( MLM + NER )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "477bd4be-8ab8-4389-b6de-08186ec33d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf<=3.20 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (3.20.0)\n",
      "Ignoring transformers: markers 'python_version < \"3.8\"' don't match your environment\n",
      "Requirement already satisfied: transformers==4.30.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (4.30.0)\n",
      "Requirement already satisfied: filelock in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from transformers==4.30.0) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2024.8.30)\n",
      "Requirement already satisfied: pytorch-crf==0.7.* in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (0.7.2)\n",
      "Requirement already satisfied: torch<1.14.0,>=1.6.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (75.1.0)\n",
      "Requirement already satisfied: wheel in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (0.44.0)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 09:21:04.92 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/v1/ner/ner_rus_bert_coll3_torch.tar.gz download because of matching hashes\n",
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "ner_model = build_model('ner_collection3_bert', download=True, install=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e8ad19c-0a67-486f-bcba-fa98c080869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Президент', 'России', 'Владимир', 'Путин', 'прибыл', 'в', 'Ташкент', '.']], [['O', 'S-LOC', 'B-PER', 'E-PER', 'O', 'O', 'S-LOC', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "preds = ner_model([\"Президент России Владимир Путин прибыл в Ташкент.\"])\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1200b74e-16d1-4dda-8b3d-0b714b0b3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned-lenta-ru-news-100k.csv\")\n",
    "full_text = df['full_text'][:10000].tolist()\n",
    "\n",
    "def prep_text(text:str):\n",
    "    return text.replace('\\xa0', ' ')\n",
    "\n",
    "full_text = list(map(prep_text, full_text))\n",
    "\n",
    "\n",
    "def split_text(text, max_tokens=512):\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for token in tqdm(tokens):\n",
    "        current_chunk.append(token)\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            chunks.append(tokenizer.convert_tokens_to_string(current_chunk))\n",
    "            current_chunk = []\n",
    "    if current_chunk:\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74f05a94-1f55-4fcd-818a-679dcd564b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 2689016/2689016 [00:02<00:00, 969133.23it/s]\n",
      "100%|███████████████████████████████████████| 5252/5252 [03:03<00:00, 28.70it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(full_text)\n",
    "\n",
    "all_preds = []\n",
    "for chunk in tqdm(chunks):\n",
    "    try:\n",
    "        preds = ner_model([chunk])\n",
    "    except:\n",
    "        continue\n",
    "    all_preds.append(preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "718e58f0-295b-41a1-9953-35df49bbc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tag(tags: list):\n",
    "    new_tag = None\n",
    "    new_tag = list(map(lambda x: x.replace('E','I').replace('S','B').replace('PIR','PER'), tags))\n",
    "    return new_tag\n",
    "\n",
    "add_dataset = {\n",
    "    'tokens': [],\n",
    "    'ner_tags': []\n",
    "}\n",
    "\n",
    "for i in all_preds:\n",
    "    ner_tag = replace_tag(i[1][0])\n",
    "    add_dataset['tokens'].append(i[0][0])\n",
    "    add_dataset['ner_tags'].append(ner_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585b6ea-0354-4134-a408-3a1f6dc29e03",
   "metadata": {},
   "source": [
    "### Перейдем к обучению на уровне MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40bb722c-7473-48b0-875c-a8e44558243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_tokens, test_data_tokens, train_data_tags, test_data_tags = train_test_split(\n",
    "    add_dataset[\"tokens\"], add_dataset[\"ner_tags\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_add_dataset = {\"tokens\": train_data_tokens, \"ner_tags\": train_data_tags}\n",
    "test_add_data_dataset = {\"tokens\": test_data_tokens, \"ner_tags\": test_data_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "800e38c1-d3cc-422c-9ca3-a154e0b53135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_add_dataset = Dataset.from_dict(train_add_dataset)\n",
    "test_add_dataset = Dataset.from_dict(test_add_data_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84b03466-3ead-4911-99c0-08fa09b05adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da703602a7ba46c8badfe2dd004e754f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bea891a43643f8ab25d9a1276c3c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_add_dataset_tok = train_add_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_add_dataset_tok = test_add_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34371085-385e-46b6-929b-c7fc4614232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-100, 0, 5, 6, 7, 8, 9, 10}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_add_dataset_tok['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97389e34-e98a-4765-a065-091de743f58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 652\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5ba29b6-8b7e-41d9-a76e-6253d9e2da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_dataset = concatenate_datasets([train_add_dataset_tok, train_dataset_tok])\n",
    "test_merged_dataset = concatenate_datasets([test_add_dataset_tok, test_dataset_tok])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcae7c-1d86-4514-9a90-9eed28b23590",
   "metadata": {},
   "source": [
    "## Обучаем на уровне MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fb04d2a-1569-420a-96bc-57c82784de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_text = [\" \".join(data['tokens']) for data in train_merged_dataset]\n",
    "test_full_text = [\" \".join(data['tokens']) for data in test_merged_dataset]\n",
    "\n",
    "train_mlm_dataset = Dataset.from_dict({\"text\": train_full_text})\n",
    "test_mlm_dataset = Dataset.from_dict({\"text\": test_full_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26b1c3af-f4f7-4685-852a-b97c36306bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627bbfe3d33246a2801cb9e53773e157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f837b8352f0f4049a0626ef06566abe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_mlm(example):\n",
    "    return tokenizer(example['text'], return_special_tokens_mask=True)\n",
    "\n",
    "train_tokenized_mlm_dataset = train_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])\n",
    "test_tokenized_mlm_dataset = test_mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "007dffc1-ac0c-411d-a209-0dbb25ffec43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519832d8e1c14c5db95a93ce0c947ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca28e33ec63f447fb9bde24b0e8ae3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "train_mlm_dataset = train_tokenized_mlm_dataset.map(group_texts, batched=True)\n",
    "test_mlm_dataset = test_tokenized_mlm_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f5bc4bf-5250-4f0f-b82a-9e60448d7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=\"./exp3-mlm-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer_mlm = Trainer(\n",
    "    model=model_mlm,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=train_mlm_dataset,\n",
    "    eval_dataset=test_mlm_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_mlm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc0689-9cc4-45b4-acca-4ab1d946bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer_mlm.evaluate()\n",
    "print(f\"Perplexity: {np.exp(eval_result['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4f320-9400-413f-b765-adfc356005b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls exp3-mlm-rubert-tiny2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed2aa5-f711-4c44-9d52-1703e61488db",
   "metadata": {},
   "source": [
    "## Перейдем к обучению в рамках NER задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5af2e6-a524-412c-ba1d-f2247593e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"exp3-mlm-rubert-tiny2/checkpoint-3540/\", \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"exp3-ner-rubert-tiny2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    # fp16=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_merged_dataset,\n",
    "    eval_dataset=test_merged_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591917d7-a216-4ebb-b81d-88c078269032",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad96a7-cd22-48af-9b02-9700a72b8a28",
   "metadata": {},
   "source": [
    "# Посмотрим что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426788d-1fb9-4cd9-a166-f5e77fc257df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"exp3-ner-rubert-tiny2/checkpoint-2232\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b31c4-44be-49b1-8f53-bf359eaca087",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"Генеральный директор Сбербанка Герман Греф на конференции в Москве заявил, что сотрудничество с Яндексом в области искусственного интеллекта выходит на новый уровень. Он также отметил, что правительство Российской Федерации поддерживает развитие цифровой экономики, особенно в рамках Евразийского экономического союза.\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "for entity in results:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f052da2-8c83-46b9-a434-65224f7b549e",
   "metadata": {},
   "source": [
    "# Сравниваем\n",
    "1) Дообучив модель на малом корпусе ( NER ):\n",
    "- precision: 0.417838\n",
    "- recall: 0.815318\n",
    "- f1: 0.552518\n",
    "- accuracy: 0.841658\n",
    "2) Обучив модель на малом корпусе ( MLM и NER ):\n",
    "- precision: 0.435142\n",
    "- recall: 0.823249\n",
    "- f1: 0.569346\n",
    "- accuracy: 0.850425\n",
    "4) Дообучив модель на куда высоком корпусе ( MLM и NER ):\n",
    "- precision: 0.793\n",
    "- recall: 0.914\n",
    "- f1: 0.849\n",
    "- accuracy: 0.972\n",
    "\n",
    "Четвертый подход показал куда высокие результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
