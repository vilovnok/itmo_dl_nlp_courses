{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bdac1c-26e0-4b09-9def-91ac0c339e72",
   "metadata": {},
   "source": [
    "# ДЗ1\n",
    "1. Загрузите набор данных lenta-ru-news с помощью библиотеки Corus для задачи классификации текстов по топикам (пригодятся атрибуты title, text, topic)\n",
    "2. Подготовьте данные к обучению:  \n",
    "    - Предобработайте данные: реализуйте оптимальную, на ваш взгляд, предобработку текстов (нормализация, очистка, стемминг/лемматизация и т.п.) и таргета.\n",
    "    - **hint**: для ускорения обработки  и обучения можно ограничиться не всем датасетом, а его репрезентативной частью, например, размера 100_000.\n",
    "    - Кратко опишите пайплайн, на котором остановились, и почему.\n",
    "    - Разделите датасет на обучающую, валидационную и тестовую выборки со стратификацией в пропорции 60/20/20. В качестве целевой переменной используйте атрибут `topic`\n",
    "3. Замерьте базовое качество с любым dummy-бейзлайном\n",
    "4. Обучите модель `sklearn.linear_model.LogisticRegression` с двумя вариантами векторизации:\n",
    "  - `sklearn.feature_extraction.text.CountVectorizer`\n",
    "  - `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "5. Попробуйте улучшить качество, подобрав оптимальные гиперпараметры трансформаций и модели на кросс-валидации\n",
    "6. Оцените качество лучшего пайплайна на отложенной выборке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cb36f7-5546-4feb-ab66-f14ce1d0e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгружаем все необходимые библиотеки в рамках данного Дз.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1f126-d49d-4c48-9595-ace28f7013fd",
   "metadata": {},
   "source": [
    "## 1. Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce78206-5e56-44f3-9f30-abaf6679582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## загружаем данные\n",
    "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c286799-a52e-4775-8cc8-70413cd001d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lenta-ru-news.csv.gz', compression='gzip')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24a2ab36-5944-4415-93d8-f5221a20e42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Россия               155078\n",
       "Мир                  136679\n",
       "Экономика             76433\n",
       "Спорт                 57902\n",
       "Культура              53536\n",
       "Наука и техника       53136\n",
       "Бывший СССР           51370\n",
       "Интернет и СМИ        44433\n",
       "Из жизни              27519\n",
       "Дом                   21734\n",
       "Силовые структуры     11223\n",
       "Ценности               7581\n",
       "Бизнес                 7375\n",
       "Путешествия            6370\n",
       "69-я параллель         1268\n",
       "Крым                    666\n",
       "Культпросвет            340\n",
       "Легпром                 114\n",
       "Библиотека               65\n",
       "Оружие                    3\n",
       "ЧМ-2014                   2\n",
       "МедНовости                1\n",
       "Сочи                      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_text'] = df['title'] + \" \" + df['text']\n",
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c89d8-413d-4257-804d-c28b8bd03720",
   "metadata": {},
   "source": [
    "Наблюдаем несбалансированную выборку, поэтому избавимся от тех классов, которые имеют меньше 2000 объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d51f068-fa36-4e43-8406-6be93965389a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(710369, 6)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts = df[\"topic\"].value_counts()\n",
    "valid_categories = category_counts[category_counts >= 2000].index\n",
    "df_filtered = df[df[\"topic\"].isin(valid_categories)].copy()\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "413a0ebf-9b9f-4a2c-a55e-e1c6b08500c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Россия               155078\n",
       "Мир                  136679\n",
       "Экономика             76433\n",
       "Спорт                 57902\n",
       "Культура              53536\n",
       "Наука и техника       53136\n",
       "Бывший СССР           51370\n",
       "Интернет и СМИ        44433\n",
       "Из жизни              27519\n",
       "Дом                   21734\n",
       "Силовые структуры     11223\n",
       "Ценности               7581\n",
       "Бизнес                 7375\n",
       "Путешествия            6370\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w = df_filtered[['full_text','topic']]\n",
    "df_w['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66f81b-8ba6-47e1-b172-14c14a350c0a",
   "metadata": {},
   "source": [
    "Возьмем для каждого класса опред. количество samples, чтобы невелировать дисбаланс классов, сохраняя при этом 100_000 объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e1285b3-1433-4f56-93d4-1c46a1da5b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotas = {\n",
    "    \"Россия\": 9_000, \n",
    "    \"Мир\": 9_000, \n",
    "    \"Экономика\": 9_000, \n",
    "    \"Спорт\": 9_000, \n",
    "    \"Культура\": 9_000, \n",
    "    \"Бывший СССР\": 9_000, \n",
    "    \"Наука и техника\": 9_000, \n",
    "    \"Интернет и СМИ\": 9_000, \n",
    "    \"Из жизни\": 5_000, \n",
    "    \"Дом\": 5_000, \n",
    "    \"Силовые структуры\": 6_000, \n",
    "    \"Ценности\": 4_000, \n",
    "    \"Бизнес\": 4_000, \n",
    "    \"Путешествия\": 4_000\n",
    "}\n",
    "\n",
    "balanced_df = pd.concat([df_w[df_w[\"topic\"] == cat].sample(n, random_state=42) for cat, n in quotas.items()])\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "balanced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f72fc8-69f1-428d-911b-c857512b84e0",
   "metadata": {},
   "source": [
    "## 2. Обрабатываем полученные данные\n",
    "Перед обучением немного преообразуем данные, а именно:\n",
    "\n",
    "- убераем html-разметку\n",
    "- приведем к нижнему регистру\n",
    "- оставляем только кириллицу\n",
    "- удаляем стоп-слова\n",
    "- убираем короткие слова (менее 3 символов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0236fe7-921e-4e13-abe2-32a833a0eca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /nfs/home/rgurtsiev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /nfs/home/rgurtsiev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "russian_stopwords = set(stopwords.words(\"russian\")) \n",
    "\n",
    "# def preprocess_text(text, lemmatize=True):\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "#     for data in soup(['style', 'script']):\n",
    "#         data.decompose()\n",
    "        \n",
    "#     text = ' '.join(soup.stripped_strings)\n",
    "#     text = ' '.join(emoji.replace_emoji(text, replace='').split())\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^а-яё ]', ' ', text)\n",
    "    \n",
    "#     tokens = word_tokenize(text)\n",
    "#     tokens = [word for word in tokens if word not in russian_stopwords]\n",
    "    \n",
    "#     if lemmatize:\n",
    "#         tokens = [mystem.lemmatize(word)[0] for word in tokens]\n",
    "    \n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\")\n",
    "    text = text.lower()    \n",
    "    text = re.sub(r'[^а-яё\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8768c99-1269-47aa-9a6a-5f23d543ef32",
   "metadata": {},
   "source": [
    "Обработаем теперь имеющий датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "360edf85-1fc6-44dc-a601-1d387981b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Text: 100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:15<00:00, 6391.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "n_jobs = 10\n",
    "\n",
    "balanced_df[\"cleaned_full_text\"] = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(preprocess_text)(text) for text in tqdm(balanced_df[\"full_text\"], desc=\"Processing Text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "851ec3ee-8c3a-487d-b590-8fd8eef203eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_df.to_csv('cleaned-lenta-ru-news-100k.csv', index=False)\n",
    "# balanced_df = pd.read_csv('cleaned-lenta-ru-news-100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "77976b06-eabc-4c32-8396-acb445608a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Экономика            9000\n",
       "Наука и техника      9000\n",
       "Бывший СССР          9000\n",
       "Спорт                9000\n",
       "Культура             9000\n",
       "Мир                  9000\n",
       "Россия               9000\n",
       "Интернет и СМИ       9000\n",
       "Силовые структуры    6000\n",
       "Из жизни             5000\n",
       "Дом                  5000\n",
       "Бизнес               4000\n",
       "Ценности             4000\n",
       "Путешествия          4000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3bf519-e286-483b-b26e-78fe8b8b5490",
   "metadata": {},
   "source": [
    "Стало лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a5734247-f3ed-4033-831e-962e6bde743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 60000\n",
      "Размер валидационной выборки: 20000\n",
      "Размер тестовой выборки: 20000\n"
     ]
    }
   ],
   "source": [
    "X = balanced_df['cleaned_full_text']\n",
    "y = balanced_df['topic']\n",
    "\n",
    "label2id = {label:id for id, label in enumerate(y.unique())}\n",
    "id2label = {id:label for id, label in enumerate(y.unique())}\n",
    "\n",
    "y = y.map(label2id)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {len(X_train)}\")\n",
    "print(f\"Размер валидационной выборки: {len(X_val)}\")\n",
    "print(f\"Размер тестовой выборки: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebc060-826f-4fae-b3eb-e8f9801c5c6a",
   "metadata": {},
   "source": [
    "# 3. Базовое качество с DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6d2e3b9d-e195-407d-931a-8aff0633b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Accuracy: 0.0900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "y_pred = dummy.predict(X_test)\n",
    "\n",
    "print(f\"Dummy Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56397095-591b-439c-be19-9f55fd85544e",
   "metadata": {},
   "source": [
    "# 4. Обучаем модель на основе другого подхода \n",
    "- CountVectorizer \n",
    "- TfidfVectorizer\n",
    "\n",
    "В качестве модели будем использовать LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "73dd23db-516c-4c69-bbc6-36d6c89a6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer accuracy: 0.7999\n",
      "TfidfVectorizer accuracy: 0.8145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe_count = Pipeline([\n",
    "    ('vec', CountVectorizer(max_features=20_000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "pipe_count.fit(X_train, y_train)\n",
    "y_pred_count = pipe_count.predict(X_val)\n",
    "\n",
    "\n",
    "pipe_tfidf = Pipeline([\n",
    "    ('vec', TfidfVectorizer(max_features=20_000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "pipe_tfidf.fit(X_train, y_train)\n",
    "y_pred_tfidf = pipe_tfidf.predict(X_val)\n",
    "\n",
    "print(f'CountVectorizer accuracy: {accuracy_score(y_val, y_pred_count):.4f}')\n",
    "print(f'TfidfVectorizer accuracy: {accuracy_score(y_val, y_pred_tfidf):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced9195-07cc-420b-b273-fbda275ed2a0",
   "metadata": {},
   "source": [
    "# 5. Попробуйте улучшить качество, подобрав оптимальные гиперпараметры трансформаций и модели на кросс-валидации\n",
    "Поскольку TF-IDF показал лучший результат, то его мы и будем улучшать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65a5ec-7c40-4537-94fd-3a00f0002af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'vec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vec__max_features': [10_000, 20_000, 30_000],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe_tfidf, params, cv=3, n_jobs=10)\n",
    "grid.fit(X_val, y_val)\n",
    "\n",
    "print(f'Best params: {grid.best_params_}')\n",
    "print(f'Best CV accuracy: {grid.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda17db-f22c-46fd-b31c-adf3e4e5a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy_score(y_test, y_test_pred):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
